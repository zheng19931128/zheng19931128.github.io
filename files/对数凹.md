
---
title: "对数凹和极大似然"
date: 2018-10-17 20:00:00
---


第一部分 凹和凸
=====

陈寅恪的“恪”字到底是读“que”还是“ke”呢，按现代汉语读音“恪”不是多音字，只有一个读音“ke”。但陈先生他老人家和他的学生们都读“que”，据说按照《广韵》的反切，在宋代时古汉语还是读“que”的，陈先生和他的学生们都是大佬，我们也就默认了。那么问题来了，贾平凹说他的凹字读“wa”。。。请打开凸优化这本书，把所有凹标注为wa，是不是挺有趣的呢？

好吧，上面这个一点也不搞笑的段子权当是学**秃**优化后幽默感骤降的表现了。凸优化在学习的时候经常缺乏动力，一是因为因为真的比较难，而是因为我真的不知道东西有啥用啊！没有用还真么难，一句“obviously”推半天，凸优化真的会变秃！

不过最近在统计中看到似然后，等到开始求极值时才恍然大悟，凸优化真的好，这是前人的结晶啊，这把瑞士军刀用起来真的十分顺手。本文就说说在误差服从正态分布时，求极大似然的一些细节，这些细节让我这个菜鸟受益很多。


凹 对数凹
----

在说凹之前得说说凸，这俩是一对兄弟，一般是可以相互转换的。

**凸集**

有一个集合 $C$ ，里面有任意两个点 $x$ , $y$, 如果这两个点之间的连线依然在 $C$ 里面， 那么这个 $C$ 就是凸的（convex）。用数学的符号表示就是：

如果 $ 0 \le \theta \le 1$ ， $ x_1, x_2 \in C$, 那么 就会有  $ \theta x_1 + (1-\theta) x_2 \in C $

那么 $C$ 就是凸的。

比如一个凸集长这样

图

**凸函数**

凸函数是定义在凸集上的。 如果 **dom** $f$ (也就函数 $f$ 的自变量)是一个凸集，任意对于所有的 
 $   x, x \in $ **dom** $f$, $0 \le \theta \le 1 $ 有
 
 $$ f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)$$
 
 那么 $f$ 就是凸函数（convex function），凸函数的极小值就是最小值，我们可以用很多方式来计算其最值，当函数可导且导数存在解析时，可以直接求导等于0求的最值。。但是如果每次见到一个函数都用这种原始的定义方式来判断，会非常的麻烦。好在前人已经证明了凸函数需要满足的一阶条件(First-order conditions)和二阶条件(Second-order-conditions)，只要证明一个函数的满足这些条件即可。同时还有很多保凸的运算如几个凸函数与非负数相乘再相加，则得到的新的函数依然是凸的。

二阶条件：

若$f$ 满足$$ \nabla ^2 f(x) \geq 0 $$，则

 
 如图的二次函数就是一个凸函数$f$是凸的。


**凹函数 和 对数凹**

凹函数和凸函数就在一个地方不一样，如果一个函数给它乘以一个负一， $-f$ 如果是 凸函数，那么 $f$ 就是凹函数（concave）。

重点来了，如果 $f$ 再定义域内都有 $f(x) >0$ ，且 $ log f $是凹的，那么 $f$ 就是对数凹（logconcave）


两个函数的凸和凹
----

**二次型**

二次型的矩阵形式可以写成 
  $ x^T Ax$
， 它可以看成可以关于 $x$ 的一个二次型，一般会用它来判断 $A$ 是否正定， 如果二次型 $ x^T Ax > 0$ 则说明 $ A$ 正定，如果 $ x^T Ax \geq 0$ 则说明 $A$ 半正定，正定矩阵有很多很好性质，比如对称，可逆等等。

判断 $ x^T Ax$ 是凹还是凸可以使用二阶条件，$ \nabla ^2 f(x) \geq 0$ 即Hessian矩阵半正定。

我们先对 $f(x)$求一阶梯度

$$
\begin{align}
\nabla f(x) &= \nabla x^TAx\\
&=(A^T+A)x
\end{align}
$$

在对$x$求二阶梯度
$$
\begin{align}
\nabla^2 f(x) &= \nabla(A + A^T)x\\
&=A^T+A
\end{align}
$$

当 $A$ 为对称矩阵时，$ \nabla ^2 f(x)=2A$。

举一个后面可以用的例子，如$$ x^T \Sigma^{-1}x $$是关于$x$的二次型，其中 $\Sigma$是一个协方差矩阵，所以 $\Sigma$为正定矩阵。可以讲$\Sigma$展开：
$$
\begin{align}
x^T\Sigma x &=x^T\Sigma\Sigma^{-1}\Sigma x \\
& = (\Sigma x)^T\Sigma^{-1}(\Sigma x)
\end{align}
$$
这说明 $ \Sigma^{-1}$也是正定的。因此$ x^T \Sigma^{-1}x $的Hessian矩阵$ \nabla ^2 f(x)=2\Sigma^{-1}$为正定矩阵，$ x^T \Sigma^{-1}x $对$x$是凸的。

**仿射**

依然是这个$ x^T Ax $， 但是这次不是二次型了，为什么呢？因为二次型是对$x$来讲,如果我们把自变量变成$A$,那么显然$ x^T Ax $对$A$是线性的。按照凸优化的划分，$ x^T Ax $是仿射（affine）的。放射又一个特点，那就是它既是凸的也是凹的。

举个例子，如果一个函数$f(\Sigma^{-1})=x^T \Sigma^{-1}x $, 则$f$是仿射，如果有一个函数$g$， 那么$g + f$是凸还是凹需要看$g$，如果$g$是凸的则$g + f$为凸，如果$g$为凹则$g + f$为凹。


**对数行列式**

对数行列式（log-determinant）是指函数$f(X)=log\,det\,X$，其中$X$是任意一个矩阵，这样的函数是凹的，也就是说$det\,X$是对数凹的。这是一个很重要的结论，下来就来证证它。

$f(X)=log\,det\,X$的$X$是一个矩阵，如果对其求二阶梯度，来判断是否符合条件是比较困难的。我们可以使用$X= Z +tV$来作替换，把自变量转换为$t$，其中$Z，V$和为矩阵，$t$为一个一维变量。这样可以把对$X$求导转换为对$t$求导，以简化问题。

首先我们来说说为什么可以这样做。根据凸函数的定义，凸函数应满足这个定义：

$$ f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)$$

我们把左项可以进行一些神奇的变换：
$$
\begin{align}
f(\theta x + (1-\theta) y) &=f(y+\theta(x-y) )\\
&=f(z +tv) \end{align}
$$
z为**dom** $f$上任意一个点，$v$与自变量纬度一样的常量，$t$是任意的实数。这就验证了我们可以使用线性替换$X= Z +tV$来简化问题。
经过替换后， 我们可以进行如下变化。


$$
\begin{align}
log\,det\,X &= log\,det\,(Z + tV)\\
&=log\,det\,(Z^{1/2}(I+tZ^{-1/2}VZ^{-1})Z^{1/2})\\
&= log\,((det\,Z^{1/2})(det\,(I+tZ^{-1/2}VZ^{-1}))det\,Z^{1/2})\\
&= log\,(det(Z^{1/2}Z^{1/2})det\,(I+tZ^{-1/2}VZ^{-1}))\\
&= log\,det(I+tZ^{-1/2}VZ^{-1/2})+log\,det\,Z
 \end{align}
$$


$log\,det\,Z$是一个常数，对$t$求导为0。经过变换之后的式子是一个关于$t$的函数，我们将其命名为$g(t)$设$\lambda_{1}...\lambda_{n}$是$tZ^{-1/2}VZ^{-1/2}$的特征根（eigenvalues)。因此化简得，

$$log\,det(I+tZ^{-1/2}VZ^{-1/2})= \sum_{i=1}^nlog(1+t\lambda_{i})$$

因此求二阶梯度可得

$$\nabla g(t)=\sum_{i=1}^n \frac{\lambda_i}{1+t\lambda_i}$$
$$\nabla^2 g(t)=-\sum_{i=1}^n \frac{\lambda_i^2}{(1+t\lambda_i)^2}$$

因此$\nabla^2 g(t)\le0$，因此$f(X)=log\,det\,X$是凹的，也可以说$det\,X$是对数凹的。


