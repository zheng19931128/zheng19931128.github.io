第二部分 极大似然
===

线性回归的误差分布：正态分布
---


[正态分布的前世今生](https://cosx.org/2013/01/story-of-normal-distribution-1)非常生动详细地讲了正态分布的发现的纠结过程。正态分布（高斯分布）的发现和线性回归问题有着非常的关系。

假设：变量$x_i$与$y$存在线性关系，则可以这样表示它们的之间的关系
$$ y=\beta_0 + \beta_1x_1 + ... +\beta_px_p $$
$\beta_i$是线性函数的参数，有$p+1$个，也是我们想要估计的参数值。现在我们收集到了一大堆数据用来拟合这些参数，当然你也知道收集的数据越多，估计的参数也会越真实。我们把收集到的数据带入等式中可以得到这样的等式。

$$y_1=\beta_0 + \beta_1x_{11} + ... +\beta_px_{p1}\\
y_2=\beta_0 + \beta_1x_{12} + ... +\beta_px_{p2}\\
.\\
.\\
.\\
y_n=\beta_0 + \beta_1x_{1n} + ... +\beta_px_{pn}
$$
当这个等式组$n=p$时，也就是说收集到的数据的组数等于参数的个数时，或者这些等式组的不等式的非线性的个数是$p$。那么这个不等式恰好有一个解。那么，当收集到很多个数据，数据的组数大于参数的个数时($p<n$)时，则该方程无解。现在我们很自然地就知道，虽然无解，但是可以找到一组$\beta$可以使模型的预测值和理论值的误差最小的参数，即

$\sum_{i=1}^n(y_i-\widehat{y})$
。$\widehat{y}$是$y$的估计。
这就是线性回归中需要优化的代价函数（costfunction）。这个代价函数事实上是根据$Y = X \beta + \epsilon$ 中 随机扰动$\epsilon$的是服从正态分布的假设中推导出来的。方法就是极大似然法（Maximum Likelihood Estimate）。 

在线性回归中使用的极大似然法简单来说就是随机误差项 每一个等式中的$\epsilon_i$服从正态分布且独立。
那么其似然函数就是将这每一个发生$\epsilon$的事件概率相乘。得到的
$$
L = \prod_{i=1}^n P(\epsilon_i)
$$
因为 $\epsilon_i = y_i - x_i^T\beta$ 且服从正态分布$N(0, \sigma^2)$, 因此可以将似然函数整理得：


$$
L(\beta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-x_i^T\beta)^2}{2\sigma^2}}
$$
通常的做法是为了简化，对等式两边同时取$ln$ 可以得到：

$$
ln L(\beta)= -n ln{\sqrt{2\pi}\sigma} +\sum_{i=1}^n{\frac{-(y_i-x_i^T\beta)^2}{2\sigma^2}}
$$
此时可以看到我们从正态分布的极大似然中得到了$-(y_i-x_i^T\beta)^2$， 即为线性回归中的代价函数。其中，$-(y_i-x_i^T\beta)^2$是一个关于$\beta$的凹函数，因为其二阶大于0，且求和为并不改变其凹性，因此$lnL(\beta)$为凹函数，存在最大的$\beta$使之达到最大。

同时$(y_i-x_i^T\beta)^2$是一个凸函数存在最小值，这就是我们一般线性回归需要最小化的代价函数。


多元正态分布的的极大似然求解
---

线性回归中的$\epsilon$是服从$N(0,\sigma^2)$的，由此根据每次误差项独立由极大似然推导出代价函数。由于$\epsilon$事实上是一维的随机变量,服从一维的正态分布，$e$ 幂上的 ${-\frac{(y_i-x_i^T\beta)^2}{2\sigma^2}}$也是一个一维的数字，此时对$\beta$求导是非常容易的，但是当随机变量是一个高纬的,$X=(x_1,x_2,...,x_n)$,若其中每一个分量$x_i$服从正态分布，且彼此间存在一定的相关性，那么$X$服从多元正太分布。

$x_i,x_j$之间的相性关系用$cov(x_i,x_j)=E(X_i-EXi)(X_j-EX_j)$来衡量。
将$cov(x_i,x_j)$按照顺序排列成矩阵，得到相关矩阵$\Sigma$。

$X$服从多元正太分布，则其概率密度函数为:
$$
y = \frac{1}{\sqrt{2\pi}}\det{\Sigma^{-1}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$

如果我们测的来多个样本,包含$X_1,X_2,...,X_n$,想要估计$X$分布的均值和方差，则可以使用极大似然法来估计。
$$
L = \prod_{i=1}^n\frac{1}{\sqrt{2\pi}}\det{\Sigma^{-1}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$

将相同项化简可以得到
$$
L = (\frac{1}{\sqrt{2\pi}})^n (\det \Sigma^{-1})^n \prod_{i=1}^{n}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
$$
对等式两边取对数，得到对数似然。

$$
\ln L = n\ln (\frac{1}{\sqrt{2\pi}}) + n \ln \det \Sigma^{-1} +  \sum_{i=1}^{n}-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)
$$

现在我们获得一批样本$X$，需要估计出$\mu, \Sigma$。此时就需要求出能让似然函数达到最大的$\mu, \Sigma$.


**$\mu$的估计**

$\ln L(\mu,\Sigma)$ 对 $\mu$ 是凹的。为什么呢？因为$\ln L$只有最后一项$\sum_{i=1}^{n}-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)$是关于$\mu$的，这是一个关于$x$的二次函数。其中$\Sigma$是正定的，因此$x^T\Sigma x \ge0$, 而且$$x^T\Sigma x = x^T\Sigma \Sigma^{-1} \Sigma x\ge0$$, 因此$\Sigma^{-1}$也是正定的。那么可以得到$\ln L$对$\mu$的$\nabla^{2}\ln L$为$-\Sigma^{-1}\le0$, 由此可以得到为凹函数，在$\nabla\ln L = 0$处存在最大值，其值为$$\mu=\frac{1}{n}\sum_{n=i}^{n}X_i$$

**$\Sigma$的估计**
$\ln L(\mu,\Sigma)$ 对 $\Sigma$ 是凹的。为什么呢？这就需要第一部分的知识了。$\ln L(\mu,\Sigma)$关于$\Sigma$有两项分别为$n \ln \det \Sigma^{-1}$和$\sum_{i=1}^{n}-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)$。上一篇我们已经证明了$\ln \det X$对$X$是一个凹函数。因此$n \ln \det \Sigma^{-1}$对$\Sigma^{-1}$是凹的。后一项$\sum_{i=1}^{n}-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)$关于$\Sigma^{-1}$是放射(affine)的，因此这一项可以是凹的也是凸的。

因此，可以对其求一阶导令其等于0求得$\Sigma^{-1}$的估计，$\Sigma^{-1}$与$\Sigma$是一一对应的可以得到对$\Sigma$也是最优的。

